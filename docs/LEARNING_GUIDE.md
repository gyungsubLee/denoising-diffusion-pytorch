# Denoising Diffusion Probabilistic Model (DDPM) - í•™ìŠµ ê°€ì´ë“œ

## ğŸ“š ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **Denoising Diffusion Probabilistic Models (DDPM)**ì˜ PyTorch êµ¬í˜„ì…ë‹ˆë‹¤. Diffusion ëª¨ë¸ì€ GANê³¼ ê²½ìŸí•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ìƒì„± ëª¨ë¸ë§ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ, ë°ì´í„° ë¶„í¬ì˜ ê¸°ìš¸ê¸°ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ denoising score matchingì„ ì‚¬ìš©í•˜ê³ , ì‹¤ì œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ Langevin samplingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

## ğŸ“– ì£¼ìš” ì°¸ê³  ë…¼ë¬¸

### 1. **í•µì‹¬ ë…¼ë¬¸: DDPM (2020)**
```bibtex
@inproceedings{NEURIPS2020_4c5bcfec,
    author      = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    title       = {Denoising Diffusion Probabilistic Models},
    booktitle   = {NeurIPS 2020},
    year        = {2020},
    url         = {https://arxiv.org/abs/2006.11239}
}
```

### 2. **ê°œì„ ëœ DDPM (2021)**
- **Improved Denoising Diffusion Probabilistic Models** (Nichol & Dhariwal, 2021)
- Cosine noise schedule ì œì•ˆ
- í•™ìŠµëœ ë¶„ì‚°(variance) ì‚¬ìš©

### 3. **DDIM (2021)**
- **Denoising Diffusion Implicit Models** (Song et al., 2021)
- ë¹ ë¥¸ ìƒ˜í”Œë§ì„ ìœ„í•œ non-Markovian í”„ë¡œì„¸ìŠ¤
- 250 ìŠ¤í…ìœ¼ë¡œ ê³ í’ˆì§ˆ ìƒ˜í”Œ ìƒì„± ê°€ëŠ¥

### 4. **ê¸°íƒ€ ì¤‘ìš” ë…¼ë¬¸**
- **Elucidating the Design Space of Diffusion Models** (Karras et al., 2022)
- **Classifier-Free Diffusion Guidance** (Ho, 2022)
- **Min-SNR Weighting Strategy** (Hang et al., 2023)

---

## ğŸ§® í•µì‹¬ ìˆ˜ì‹ ë° êµ¬í˜„

### 1. **Forward Diffusion Process (ìˆœë°©í–¥ í™•ì‚°)**

Forward processëŠ” ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

#### ìˆ˜ì‹

$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} \cdot x_{t-1}, \beta_t \cdot I)$$

ì—¬ê¸°ì„œ:
- $x_t$: ì‹œê°„ tì—ì„œì˜ noisy ì´ë¯¸ì§€
- $\beta_t$: ì‹œê°„ tì—ì„œì˜ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (variance schedule)
- $\mathcal{N}$: ê°€ìš°ì‹œì•ˆ ë¶„í¬

#### ì¤‘ìš”í•œ ì„±ì§ˆ: Closed-form ìƒ˜í”Œë§

$$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} \cdot x_0, (1 - \bar{\alpha}_t) \cdot I)$$

ì—¬ê¸°ì„œ $\bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i)$ (alpha cumulative product)

#### ì¬ë§¤ê°œë³€ìˆ˜í™” (Reparameterization)

$$x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

#### ì½”ë“œ êµ¬í˜„

```python
# denoising_diffusion_pytorch.py:787-793
def q_sample(self, x_start, t, noise = None):
    noise = default(noise, lambda: torch.randn_like(x_start))

    return (
        extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +
        extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise
    )
```

**ìˆ˜ì‹ ëŒ€ì‘:**
- `sqrt_alphas_cumprod` = $\sqrt{\bar{\alpha}_t}$
- `sqrt_one_minus_alphas_cumprod` = $\sqrt{1 - \bar{\alpha}_t}$
- `x_start` = $x_0$
- `noise` = $\epsilon$

---

### 2. **Reverse Diffusion Process (ì—­ë°©í–¥ í™•ì‚°)**

Reverse processëŠ” ë…¸ì´ì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

#### ìˆ˜ì‹

$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

#### Posterior ë¶„í¬

ì›ë³¸ ì´ë¯¸ì§€ $x_0$ë¥¼ ì•Œ ë•Œì˜ posterior:

$$q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t \cdot I)$$

ì—¬ê¸°ì„œ:

$$\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \cdot \beta_t}{1 - \bar{\alpha}_t} \cdot x_0 + \frac{\sqrt{\alpha_t} \cdot (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \cdot x_t$$

$$\tilde{\beta}_t = \frac{(1 - \bar{\alpha}_{t-1})}{(1 - \bar{\alpha}_t)} \cdot \beta_t$$

#### ì½”ë“œ êµ¬í˜„

```python
# denoising_diffusion_pytorch.py:646-653
def q_posterior(self, x_start, x_t, t):
    posterior_mean = (
        extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +
        extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
    )
    posterior_variance = extract(self.posterior_variance, t, x_t.shape)
    posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)
    return posterior_mean, posterior_variance, posterior_log_variance_clipped
```

**ì´ˆê¸°í™” ì‹œ ê³„ì‚° (denoising_diffusion_pytorch.py:577-589):**

```python
# Î²Ìƒ_t ê³„ì‚°
posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

# Î¼Ìƒ_tì˜ ê³„ìˆ˜ë“¤
posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)
posterior_mean_coef2 = (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)
```

---

### 3. **Training Objective (í•™ìŠµ ëª©í‘œ)**

#### ì„¸ ê°€ì§€ í•™ìŠµ ëª©í‘œ

ì´ êµ¬í˜„ì²´ëŠ” ì„¸ ê°€ì§€ í•™ìŠµ ëª©í‘œë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

**1) Noise Prediction (Îµ-prediction)**

$$\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]$$

**2) x_0 Prediction**

$$\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \|x_0 - \hat{x}_\theta(x_t, t)\|^2 \right]$$

**3) v-Prediction** (Progressive Distillation ë…¼ë¬¸, Imagen-Videoì—ì„œ ì‚¬ìš©)

$$v_t = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1 - \bar{\alpha}_t} \cdot x_0$$

$$\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \|v_t - v_\theta(x_t, t)\|^2 \right]$$

#### ì½”ë“œ êµ¬í˜„

```python
# denoising_diffusion_pytorch.py:795-840
def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):
    b, c, h, w = x_start.shape
    noise = default(noise, lambda: torch.randn_like(x_start))

    # Forward process: x_0 â†’ x_t
    x = self.q_sample(x_start = x_start, t = t, noise = noise)

    # ëª¨ë¸ ì˜ˆì¸¡
    model_out = self.model(x, t, x_self_cond)

    # Objectiveì— ë”°ë¼ target ì„¤ì •
    if self.objective == 'pred_noise':
        target = noise
    elif self.objective == 'pred_x0':
        target = x_start
    elif self.objective == 'pred_v':
        v = self.predict_v(x_start, t, noise)
        target = v

    # MSE Loss
    loss = F.mse_loss(model_out, target, reduction = 'none')
    loss = reduce(loss, 'b ... -> b', 'mean')

    # Loss weighting (Min-SNR)
    loss = loss * extract(self.loss_weight, t, loss.shape)
    return loss.mean()
```

---

### 4. **Noise Schedules (ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„)**

ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ì€ í™•ì‚° ê³¼ì •ì˜ ì†ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.

#### Linear Schedule (ì›ë³¸ DDPM)

```python
# denoising_diffusion_pytorch.py:462-469
def linear_beta_schedule(timesteps):
    scale = 1000 / timesteps
    beta_start = scale * 0.0001
    beta_end = scale * 0.02
    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)
```

**ìˆ˜ì‹:**

$$\beta_t = \beta_{\text{start}} + (\beta_{\text{end}} - \beta_{\text{start}}) \cdot \frac{t}{T}$$

#### Cosine Schedule (Improved DDPM)

```python
# denoising_diffusion_pytorch.py:471-481
def cosine_beta_schedule(timesteps, s = 0.008):
    steps = timesteps + 1
    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps
    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0, 0.999)
```

**ìˆ˜ì‹:**

$$\bar{\alpha}_t = \frac{\cos^2\left(\frac{t/T + s}{1 + s} \cdot \frac{\pi}{2}\right)}{\cos^2\left(\frac{s}{1 + s} \cdot \frac{\pi}{2}\right)}$$

$$\beta_t = 1 - \frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}$$

#### Sigmoid Schedule

ê³ í•´ìƒë„ ì´ë¯¸ì§€(>64x64)ì— ë” íš¨ê³¼ì :

```python
# denoising_diffusion_pytorch.py:483-496
def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):
    steps = timesteps + 1
    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps
    v_start = torch.tensor(start / tau).sigmoid()
    v_end = torch.tensor(end / tau).sigmoid()
    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0, 0.999)
```

---

### 5. **Sampling Methods (ìƒ˜í”Œë§ ë°©ë²•)**

#### DDPM Sampling (Ancestral Sampling)

```python
# denoising_diffusion_pytorch.py:700-716
@torch.inference_mode()
def p_sample_loop(self, shape, return_all_timesteps = False):
    img = torch.randn(shape, device = device)

    for t in reversed(range(0, self.num_timesteps)):
        self_cond = x_start if self.self_condition else None
        img, x_start = self.p_sample(img, t, self_cond)

    return self.unnormalize(img)
```

**ìˆ˜ì‹ (p_sample):**

$$
x_{t-1} =
\begin{cases}
\mu_\theta(x_t, t) + \exp(0.5 \cdot \log \sigma^2_\theta(x_t, t)) \cdot z, & \text{if } t > 0 \\
\mu_\theta(x_t, t), & \text{if } t = 0
\end{cases}
$$

ì—¬ê¸°ì„œ $z \sim \mathcal{N}(0, I)$

#### DDIM Sampling (ë¹ ë¥¸ ìƒ˜í”Œë§)

```python
# denoising_diffusion_pytorch.py:719-758
@torch.inference_mode()
def ddim_sample(self, shape, return_all_timesteps = False):
    # Accelerated sampling with fewer timesteps
    times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)
    times = list(reversed(times.int().tolist()))
    time_pairs = list(zip(times[:-1], times[1:]))

    for time, time_next in time_pairs:
        pred_noise, x_start = self.model_predictions(...)

        alpha = self.alphas_cumprod[time]
        alpha_next = self.alphas_cumprod[time_next]

        sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()
        c = (1 - alpha_next - sigma ** 2).sqrt()

        img = x_start * alpha_next.sqrt() + c * pred_noise + sigma * noise
```

**DDIM ìˆ˜ì‹:**

$$x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \cdot \hat{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \hat{\epsilon}_\theta(x_t, t) + \sigma_t \cdot z$$

ì—¬ê¸°ì„œ:

$$\sigma_t = \eta \cdot \sqrt{\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}} \cdot \sqrt{1 - \frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}}$$

- $\eta = 0$ ì´ë©´ deterministic
- $\eta = 1$ ì´ë©´ DDPMê³¼ ë™ì¼

---

### 6. **Loss Weighting (Min-SNR)**

**Min-SNR Weighting Strategy** (Hang et al., 2023) ì ìš©

#### SNR (Signal-to-Noise Ratio)

$$\text{SNR}(t) = \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t}$$

#### Loss Weight ê³„ì‚°

```python
# denoising_diffusion_pytorch.py:595-611
snr = alphas_cumprod / (1 - alphas_cumprod)

maybe_clipped_snr = snr.clone()
if min_snr_loss_weight:
    maybe_clipped_snr.clamp_(max = min_snr_gamma)  # default: Î³=5

if objective == 'pred_noise':
    loss_weight = maybe_clipped_snr / snr
elif objective == 'pred_x0':
    loss_weight = maybe_clipped_snr
elif objective == 'pred_v':
    loss_weight = maybe_clipped_snr / (snr + 1)
```

**ìˆ˜ì‹:**
- Noise prediction: $w_t = \frac{\min(\text{SNR}(t), \gamma)}{\text{SNR}(t)}$
- x_0 prediction: $w_t = \min(\text{SNR}(t), \gamma)$
- v prediction: $w_t = \frac{\min(\text{SNR}(t), \gamma)}{\text{SNR}(t) + 1}$

---

### 7. **x_0 Reconstruction (ì›ë³¸ ì´ë¯¸ì§€ ë³µì›)**

ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œë¶€í„° ì›ë³¸ ì´ë¯¸ì§€ $x_0$ë¥¼ ë³µì›í•˜ëŠ” ë°©ë²•:

#### Noise Prediction â†’ x_0

```python
# denoising_diffusion_pytorch.py:622-626
def predict_start_from_noise(self, x_t, t, noise):
    return (
        extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -
        extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise
    )
```

**ìˆ˜ì‹:**

$$\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \cdot x_t - \sqrt{\frac{1}{\bar{\alpha}_t} - 1} \cdot \hat{\epsilon}_\theta(x_t, t)$$

#### v-Prediction â†’ x_0

```python
# denoising_diffusion_pytorch.py:640-644
def predict_start_from_v(self, x_t, t, v):
    return (
        extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -
        extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v
    )
```

**ìˆ˜ì‹:**

$$v_t = \sqrt{\bar{\alpha}_t} \cdot \epsilon - \sqrt{1 - \bar{\alpha}_t} \cdot x_0$$

$$\hat{x}_0 = \sqrt{\bar{\alpha}_t} \cdot x_t - \sqrt{1 - \bar{\alpha}_t} \cdot v_\theta(x_t, t)$$

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### U-Net êµ¬ì¡°

ì´ êµ¬í˜„ì€ **U-Net** ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

1. **Time Embedding**: Sinusoidal positional embedding
   ```python
   # denoising_diffusion_pytorch.py:117-130
   class SinusoidalPosEmb(Module):
       def forward(self, t):
           half_dim = self.dim // 2
           emb = math.log(self.theta) / (half_dim - 1)
           emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
           emb = t[:, None] * emb[None, :]
           emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
           return emb
   ```

2. **ResNet Block**: Wide ResNet ìŠ¤íƒ€ì¼
   - Time embeddingì„ scale & shiftë¡œ ì£¼ì…
   - RMSNorm ì‚¬ìš©

3. **Attention**: Multi-head self-attention
   - Flash Attention ì§€ì› (`flash_attn=True`)

4. **Up/Downsampling**:
   - Downsample: Pixel unshuffle (2Ã—2 â†’ 1Ã—1, 4Ã— channels)
   - Upsample: Nearest neighbor + Conv2d

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ í•™ìŠµ

```python
from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer

model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8),
    flash_attn = True
)

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,           # T=1000
    sampling_timesteps = 250,   # DDIM: 250 steps
    objective = 'pred_v',       # v-prediction
    beta_schedule = 'sigmoid',  # sigmoid schedule
    min_snr_loss_weight = True, # Min-SNR weighting
    min_snr_gamma = 5
)

trainer = Trainer(
    diffusion,
    'path/to/images',
    train_batch_size = 32,
    train_lr = 8e-5,
    train_num_steps = 700000,
    gradient_accumulate_every = 2,
    ema_decay = 0.995,
    amp = True,
    calculate_fid = True
)

trainer.train()
```

### ìƒ˜í”Œë§

```python
# DDIM ìƒ˜í”Œë§ (ë¹ ë¦„)
sampled_images = diffusion.sample(batch_size = 4)

# ëª¨ë“  timestep ë°˜í™˜
all_timesteps = diffusion.sample(batch_size = 4, return_all_timesteps = True)
```

---

## ğŸ”¬ ì£¼ìš” ê¸°ëŠ¥

### 1. Self-Conditioning
- 50% í™•ë¥ ë¡œ ì´ì „ ì˜ˆì¸¡ $\hat{x}_0$ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©
- FID ê°œì„ , í•™ìŠµ ì‹œê°„ 25% ì¦ê°€

### 2. Offset Noise
- ë°ê¸° ì¡°ì ˆ ê°œì„ 
- `offset_noise_strength = 0.1` ê¶Œì¥

### 3. EMA (Exponential Moving Average)
- ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì´ë™ í‰ê·  ìœ ì§€
- `ema_decay = 0.995`

### 4. Mixed Precision Training
- `amp = True`ë¡œ í™œì„±í™”
- ë©”ëª¨ë¦¬ ì ˆì•½, í•™ìŠµ ì†ë„ í–¥ìƒ

---

## ğŸ“Š í‰ê°€ ì§€í‘œ

### FID (FrÃ©chet Inception Distance)

```python
trainer = Trainer(
    diffusion,
    ...,
    calculate_fid = True,
    fid_every = 1000  # 1000 ìŠ¤í…ë§ˆë‹¤ FID ê³„ì‚°
)
```

---

## ğŸ“ í•™ìŠµ íŒ

1. **ì‹œì‘ ì„¤ì •**:
   - `objective = 'pred_v'`
   - `beta_schedule = 'sigmoid'` (ê³ í•´ìƒë„) ë˜ëŠ” `'cosine'` (ì €í•´ìƒë„)
   - `min_snr_loss_weight = True`

2. **í•™ìŠµ ì•ˆì •ì„±**:
   - Gradient accumulation ì‚¬ìš©
   - EMA decay 0.995-0.9999
   - Learning rate: 8e-5

3. **ë¹ ë¥¸ ìƒ˜í”Œë§**:
   - DDIM: `sampling_timesteps = 250`
   - `ddim_sampling_eta = 0.0` (deterministic)

4. **ë©”ëª¨ë¦¬ ìµœì í™”**:
   - Mixed precision (`amp = True`)
   - Gradient checkpointing
   - Flash Attention

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### YouTube ê°•ì˜
- [Yannic Kilcher](https://www.youtube.com/watch?v=W-O7AZNzbzQ)
- [AI Coffeebreak with Letitia](https://www.youtube.com/watch?v=344w5h24-h8)
- [Outlier](https://www.youtube.com/watch?v=HoKDTa5jHvg)

### ê³µì‹ êµ¬í˜„
- [TensorFlow ì›ë³¸](https://github.com/hojonathanho/diffusion)
- [HuggingFace Annotated Code](https://huggingface.co/blog/annotated-diffusion)

---

## ğŸ”‘ í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ìˆ˜ì‹ | ì½”ë“œ ìœ„ì¹˜ |
|------|------|-----------|
| Forward process | $x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1-\bar{\alpha}_t} \cdot \epsilon$ | `q_sample()` |
| Reverse process | $p_\theta(x_{t-1}\|x_t)$ | `p_sample()` |
| Loss (noise) | $\|\|\epsilon - \epsilon_\theta(x_t, t)\|\|^2$ | `p_losses()` |
| Loss (v) | $\|\|v - v_\theta(x_t, t)\|\|^2$ | `p_losses()` |
| DDIM | $x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \cdot \hat{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \cdot \hat{\epsilon}$ | `ddim_sample()` |
| SNR | $\bar{\alpha}_t / (1 - \bar{\alpha}_t)$ | Loss weighting |

---

**Happy Diffusing! ğŸ¨**
